{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 라이브러리 import\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil import parser\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine, text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MariaDB에 연결하기\n",
    "db_id = ''\n",
    "db_pw = ''\n",
    "db_ip = ''\n",
    "db_port = 0\n",
    "db_name = ''\n",
    "\n",
    "engine = create_engine(f\"mysql+pymysql://{db_id}:{db_pw}@{db_ip}:{db_port}/{db_name}?charset=utf8mb4\")\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 크롤링 함수\n",
    "def crawler(company_code, start_date, end_date):\n",
    "    page = 1\n",
    "    df_result = None\n",
    "\n",
    "    while True:\n",
    "        # URL 생성\n",
    "        url = f'https://finance.naver.com/item/news_news.nhn?code={company_code}&page={page}'\n",
    "        # HTML 소스 가져오기\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()  # HTTP 오류 발생 시 예외 처리\n",
    "            source_code = response.text\n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"[{company_code}] HTTP 오류 ({e.response.status_code}): {e}\")\n",
    "            return pd.DataFrame()  # 데이터 수집 실패 시 빈 DataFrame 반환\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"[{company_code}] 연결 오류 발생 - 페이지 {page}\")\n",
    "            continue\n",
    "\n",
    "        # BeautifulSoup 객체 생성\n",
    "        html = BeautifulSoup(source_code, \"lxml\")\n",
    "\n",
    "        # 기사 제목, 링크, 날짜 추출\n",
    "        titles = html.select('.title')\n",
    "        title_result = [title.get_text(strip=True) for title in titles]\n",
    "\n",
    "        links = html.select('.title')\n",
    "        link_result = ['https://finance.naver.com' + link.find('a')['href'] for link in links]\n",
    "\n",
    "        dates = html.select('.date')\n",
    "        date_result = [date.get_text(strip=True) for date in dates]\n",
    "\n",
    "        # 문자열 날짜를 datetime 객체로 변환\n",
    "        converted_dates = []\n",
    "        for date_str in date_result:\n",
    "            try:\n",
    "                converted_date = parser.parse(date_str).date()\n",
    "                converted_dates.append(converted_date)\n",
    "            except ValueError:\n",
    "                print(f\"[{company_code}] 날짜 변환 오류: {date_str}\")\n",
    "\n",
    "        # 결과 DataFrame 생성\n",
    "        result = {\"dates\": converted_dates, \"title\": title_result, \"link\": link_result}\n",
    "        df_page = pd.DataFrame(result)\n",
    "\n",
    "        # 날짜 형식 변환 및 비교\n",
    "        df_page['dates'] = pd.to_datetime(df_page['dates'])\n",
    "        df_page['dates'] = df_page['dates'].dt.date\n",
    "\n",
    "        df_page = df_page[(df_page['dates'] >= start_date) & (df_page['dates'] <= end_date)]\n",
    "\n",
    "        # 페이지 정보 출력 및 다음 페이지 진행\n",
    "        if not df_page.empty:\n",
    "            print(f\"[{company_code}] 페이지 {page} 다운로드 완료\")\n",
    "            page += 1\n",
    "        else:\n",
    "            print(f\"[{company_code}] 더 이상 뉴스가 없습니다.\")\n",
    "            break\n",
    "\n",
    "        # 결과 DataFrame 누적\n",
    "        if df_result is None:\n",
    "            df_result = df_page\n",
    "        else:\n",
    "            df_result = pd.concat([df_result, df_page], ignore_index=True)\n",
    "\n",
    "    return df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # 종목 코드 목록 읽어오기\n",
    "    df = ['005490','247540']\n",
    "    # ['종목코드'].tolist()\n",
    "    company_codes = df\n",
    "    codes = list(map(lambda x: str(x).zfill(6), company_codes))\n",
    "\n",
    "    # 종료 날짜 설정\n",
    "    now = datetime.now()\n",
    "    formatted_now = now.strftime('%Y%m%d')\n",
    "    end_date = datetime.strptime(formatted_now.replace('.', ''), '%Y%m%d').date()\n",
    "\n",
    "    # 시작 날짜 설정 (1년 6개월 전)\n",
    "    # start_date = end_date - timedelta(days=int(1.8*365/2))\n",
    "    start_date = end_date - timedelta(days=90)\n",
    "    # 저장 경로 설정\n",
    "    save_dir = './종목별_뉴스'\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "    # 로그 파일 설정\n",
    "    log_file = open(os.path.join(save_dir, 'log.txt'), 'w')\n",
    "\n",
    "    # 종목별 뉴스 데이터 크롤링 및 저장\n",
    "    for company_code in codes:\n",
    "        print(f\"\\n{company_code} 종목 뉴스 크롤링 시작...\")\n",
    "        df_temp = crawler(company_code, start_date, end_date)\n",
    "\n",
    "        # 빈 DataFrame 처리\n",
    "        if df_temp is None:\n",
    "            print(f\"[{company_code}] 크롤링 데이터 없음\")\n",
    "            continue\n",
    "\n",
    "        # 저장 파일명 생성\n",
    "        save_file = os.path.join(save_dir, f'{company_code}_뉴스_{start_date:%Y%m%d}_{end_date:%Y%m%d}.csv')\n",
    "\n",
    "        # 데이터 저장\n",
    "        try:\n",
    "            df_temp.insert(1,'ticker', company_code)\n",
    "            df_temp['ticker'] = df_temp['ticker'].apply(lambda x :'A' + str(x))\n",
    "            df_temp['dates'] = df_temp['dates'].apply(lambda x : datetime.strftime(x , '%y-%m-%d'))\n",
    "            df_temp.to_sql('news', conn, if_exists='append', index=False)\n",
    "            # df_temp.info()\n",
    "            conn.commit()\n",
    "            df_temp.to_csv(save_file, index=False)\n",
    "            print(f\"[{company_code}] 뉴스 데이터 저장 완료: {save_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[{company_code}] 데이터 저장 오류: {e}\")\n",
    "            log_file.write(f\"{company_code}: {e}\\n\")\n",
    "\n",
    "    # 로그 파일 닫기\n",
    "    log_file.close()\n",
    "    conn.close()\n",
    "    print(\"\\n모든 종목의 뉴스 데이터 크롤링 및 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "005490 종목 뉴스 크롤링 시작...\n",
      "[005490] 페이지 1 다운로드 완료\n",
      "[005490] 페이지 2 다운로드 완료\n",
      "[005490] 페이지 3 다운로드 완료\n",
      "[005490] 페이지 4 다운로드 완료\n",
      "[005490] 페이지 5 다운로드 완료\n",
      "[005490] 페이지 6 다운로드 완료\n",
      "[005490] 페이지 7 다운로드 완료\n",
      "[005490] 페이지 8 다운로드 완료\n",
      "[005490] 페이지 9 다운로드 완료\n",
      "[005490] 페이지 10 다운로드 완료\n",
      "[005490] 페이지 11 다운로드 완료\n",
      "[005490] 페이지 12 다운로드 완료\n",
      "[005490] 페이지 13 다운로드 완료\n",
      "[005490] 페이지 14 다운로드 완료\n",
      "[005490] 페이지 15 다운로드 완료\n",
      "[005490] 페이지 16 다운로드 완료\n",
      "[005490] 페이지 17 다운로드 완료\n",
      "[005490] 페이지 18 다운로드 완료\n",
      "[005490] 페이지 19 다운로드 완료\n",
      "[005490] 페이지 20 다운로드 완료\n",
      "[005490] 페이지 21 다운로드 완료\n",
      "[005490] 페이지 22 다운로드 완료\n",
      "[005490] 페이지 23 다운로드 완료\n",
      "[005490] 페이지 24 다운로드 완료\n",
      "[005490] 페이지 25 다운로드 완료\n",
      "[005490] 페이지 26 다운로드 완료\n",
      "[005490] 페이지 27 다운로드 완료\n",
      "[005490] 페이지 28 다운로드 완료\n",
      "[005490] 페이지 29 다운로드 완료\n",
      "[005490] 페이지 30 다운로드 완료\n",
      "[005490] 페이지 31 다운로드 완료\n",
      "[005490] 페이지 32 다운로드 완료\n",
      "[005490] 페이지 33 다운로드 완료\n",
      "[005490] 페이지 34 다운로드 완료\n",
      "[005490] 페이지 35 다운로드 완료\n",
      "[005490] 페이지 36 다운로드 완료\n",
      "[005490] 페이지 37 다운로드 완료\n",
      "[005490] 페이지 38 다운로드 완료\n",
      "[005490] 페이지 39 다운로드 완료\n",
      "[005490] 페이지 40 다운로드 완료\n",
      "[005490] 페이지 41 다운로드 완료\n",
      "[005490] 페이지 42 다운로드 완료\n",
      "[005490] 페이지 43 다운로드 완료\n",
      "[005490] 페이지 44 다운로드 완료\n",
      "[005490] 페이지 45 다운로드 완료\n",
      "[005490] 페이지 46 다운로드 완료\n",
      "[005490] 페이지 47 다운로드 완료\n",
      "[005490] 페이지 48 다운로드 완료\n",
      "[005490] 페이지 49 다운로드 완료\n",
      "[005490] 페이지 50 다운로드 완료\n",
      "[005490] 페이지 51 다운로드 완료\n",
      "[005490] 페이지 52 다운로드 완료\n",
      "[005490] 페이지 53 다운로드 완료\n",
      "[005490] 페이지 54 다운로드 완료\n",
      "[005490] 페이지 55 다운로드 완료\n",
      "[005490] 페이지 56 다운로드 완료\n",
      "[005490] 페이지 57 다운로드 완료\n",
      "[005490] 페이지 58 다운로드 완료\n",
      "[005490] 페이지 59 다운로드 완료\n",
      "[005490] 페이지 60 다운로드 완료\n",
      "[005490] 페이지 61 다운로드 완료\n",
      "[005490] 페이지 62 다운로드 완료\n",
      "[005490] 페이지 63 다운로드 완료\n",
      "[005490] 페이지 64 다운로드 완료\n",
      "[005490] 페이지 65 다운로드 완료\n",
      "[005490] 페이지 66 다운로드 완료\n",
      "[005490] 페이지 67 다운로드 완료\n",
      "[005490] 페이지 68 다운로드 완료\n",
      "[005490] 페이지 69 다운로드 완료\n",
      "[005490] 페이지 70 다운로드 완료\n",
      "[005490] 페이지 71 다운로드 완료\n",
      "[005490] 페이지 72 다운로드 완료\n",
      "[005490] 페이지 73 다운로드 완료\n",
      "[005490] 페이지 74 다운로드 완료\n",
      "[005490] 페이지 75 다운로드 완료\n",
      "[005490] 페이지 76 다운로드 완료\n",
      "[005490] 페이지 77 다운로드 완료\n",
      "[005490] 페이지 78 다운로드 완료\n",
      "[005490] 페이지 79 다운로드 완료\n",
      "[005490] 페이지 80 다운로드 완료\n",
      "[005490] 페이지 81 다운로드 완료\n",
      "[005490] 페이지 82 다운로드 완료\n",
      "[005490] 페이지 83 다운로드 완료\n",
      "[005490] 페이지 84 다운로드 완료\n",
      "[005490] 페이지 85 다운로드 완료\n",
      "[005490] 페이지 86 다운로드 완료\n",
      "[005490] 페이지 87 다운로드 완료\n",
      "[005490] 페이지 88 다운로드 완료\n",
      "[005490] 페이지 89 다운로드 완료\n",
      "[005490] 페이지 90 다운로드 완료\n",
      "[005490] 페이지 91 다운로드 완료\n",
      "[005490] 페이지 92 다운로드 완료\n",
      "[005490] 페이지 93 다운로드 완료\n",
      "[005490] 페이지 94 다운로드 완료\n",
      "[005490] 페이지 95 다운로드 완료\n",
      "[005490] 페이지 96 다운로드 완료\n",
      "[005490] 페이지 97 다운로드 완료\n",
      "[005490] 페이지 98 다운로드 완료\n",
      "[005490] 페이지 99 다운로드 완료\n",
      "[005490] 페이지 100 다운로드 완료\n",
      "[005490] 페이지 101 다운로드 완료\n",
      "[005490] 페이지 102 다운로드 완료\n",
      "[005490] 페이지 103 다운로드 완료\n",
      "[005490] 페이지 104 다운로드 완료\n",
      "[005490] 페이지 105 다운로드 완료\n",
      "[005490] 페이지 106 다운로드 완료\n",
      "[005490] 페이지 107 다운로드 완료\n",
      "[005490] 페이지 108 다운로드 완료\n",
      "[005490] 페이지 109 다운로드 완료\n",
      "[005490] 페이지 110 다운로드 완료\n",
      "[005490] 페이지 111 다운로드 완료\n",
      "[005490] 페이지 112 다운로드 완료\n",
      "[005490] 페이지 113 다운로드 완료\n",
      "[005490] 페이지 114 다운로드 완료\n",
      "[005490] 페이지 115 다운로드 완료\n",
      "[005490] 페이지 116 다운로드 완료\n",
      "[005490] 페이지 117 다운로드 완료\n",
      "[005490] 페이지 118 다운로드 완료\n",
      "[005490] 페이지 119 다운로드 완료\n",
      "[005490] 페이지 120 다운로드 완료\n",
      "[005490] 페이지 121 다운로드 완료\n",
      "[005490] 페이지 122 다운로드 완료\n",
      "[005490] 페이지 123 다운로드 완료\n",
      "[005490] 페이지 124 다운로드 완료\n",
      "[005490] 페이지 125 다운로드 완료\n",
      "[005490] 페이지 126 다운로드 완료\n",
      "[005490] 페이지 127 다운로드 완료\n",
      "[005490] 페이지 128 다운로드 완료\n",
      "[005490] 페이지 129 다운로드 완료\n",
      "[005490] 페이지 130 다운로드 완료\n",
      "[005490] 페이지 131 다운로드 완료\n",
      "[005490] 페이지 132 다운로드 완료\n",
      "[005490] 페이지 133 다운로드 완료\n",
      "[005490] 페이지 134 다운로드 완료\n",
      "[005490] 페이지 135 다운로드 완료\n",
      "[005490] 페이지 136 다운로드 완료\n",
      "[005490] 페이지 137 다운로드 완료\n",
      "[005490] 페이지 138 다운로드 완료\n",
      "[005490] 페이지 139 다운로드 완료\n",
      "[005490] 페이지 140 다운로드 완료\n",
      "[005490] 페이지 141 다운로드 완료\n",
      "[005490] 페이지 142 다운로드 완료\n",
      "[005490] 페이지 143 다운로드 완료\n",
      "[005490] 페이지 144 다운로드 완료\n",
      "[005490] 페이지 145 다운로드 완료\n",
      "[005490] 페이지 146 다운로드 완료\n",
      "[005490] 페이지 147 다운로드 완료\n",
      "[005490] 페이지 148 다운로드 완료\n",
      "[005490] 페이지 149 다운로드 완료\n",
      "[005490] 페이지 150 다운로드 완료\n",
      "[005490] 페이지 151 다운로드 완료\n",
      "[005490] 페이지 152 다운로드 완료\n",
      "[005490] 페이지 153 다운로드 완료\n",
      "[005490] 페이지 154 다운로드 완료\n",
      "[005490] 페이지 155 다운로드 완료\n",
      "[005490] 페이지 156 다운로드 완료\n",
      "[005490] 페이지 157 다운로드 완료\n",
      "[005490] 페이지 158 다운로드 완료\n",
      "[005490] 페이지 159 다운로드 완료\n",
      "[005490] 페이지 160 다운로드 완료\n",
      "[005490] 페이지 161 다운로드 완료\n",
      "[005490] 페이지 162 다운로드 완료\n",
      "[005490] 페이지 163 다운로드 완료\n",
      "[005490] 페이지 164 다운로드 완료\n",
      "[005490] 페이지 165 다운로드 완료\n",
      "[005490] 페이지 166 다운로드 완료\n",
      "[005490] 페이지 167 다운로드 완료\n",
      "[005490] 페이지 168 다운로드 완료\n",
      "[005490] 페이지 169 다운로드 완료\n",
      "[005490] 페이지 170 다운로드 완료\n",
      "[005490] 페이지 171 다운로드 완료\n",
      "[005490] 페이지 172 다운로드 완료\n",
      "[005490] 페이지 173 다운로드 완료\n",
      "[005490] 페이지 174 다운로드 완료\n",
      "[005490] 페이지 175 다운로드 완료\n",
      "[005490] 페이지 176 다운로드 완료\n",
      "[005490] 페이지 177 다운로드 완료\n",
      "[005490] 페이지 178 다운로드 완료\n",
      "[005490] 페이지 179 다운로드 완료\n",
      "[005490] 페이지 180 다운로드 완료\n",
      "[005490] 페이지 181 다운로드 완료\n",
      "[005490] 페이지 182 다운로드 완료\n",
      "[005490] 페이지 183 다운로드 완료\n",
      "[005490] 페이지 184 다운로드 완료\n",
      "[005490] 페이지 185 다운로드 완료\n",
      "[005490] 페이지 186 다운로드 완료\n",
      "[005490] 페이지 187 다운로드 완료\n",
      "[005490] 페이지 188 다운로드 완료\n",
      "[005490] 페이지 189 다운로드 완료\n",
      "[005490] 페이지 190 다운로드 완료\n",
      "[005490] 페이지 191 다운로드 완료\n",
      "[005490] 페이지 192 다운로드 완료\n",
      "[005490] 페이지 193 다운로드 완료\n",
      "[005490] 페이지 194 다운로드 완료\n",
      "[005490] 페이지 195 다운로드 완료\n",
      "[005490] 페이지 196 다운로드 완료\n",
      "[005490] 페이지 197 다운로드 완료\n",
      "[005490] 페이지 198 다운로드 완료\n",
      "[005490] 페이지 199 다운로드 완료\n",
      "[005490] 페이지 200 다운로드 완료\n",
      "[005490] 페이지 201 다운로드 완료\n",
      "[005490] 페이지 202 다운로드 완료\n",
      "[005490] 페이지 203 다운로드 완료\n",
      "[005490] 페이지 204 다운로드 완료\n",
      "[005490] 페이지 205 다운로드 완료\n",
      "[005490] 페이지 206 다운로드 완료\n",
      "[005490] 페이지 207 다운로드 완료\n",
      "[005490] 페이지 208 다운로드 완료\n",
      "[005490] 페이지 209 다운로드 완료\n",
      "[005490] 페이지 210 다운로드 완료\n",
      "[005490] 페이지 211 다운로드 완료\n",
      "[005490] 페이지 212 다운로드 완료\n",
      "[005490] 페이지 213 다운로드 완료\n",
      "[005490] 페이지 214 다운로드 완료\n",
      "[005490] 페이지 215 다운로드 완료\n",
      "[005490] 페이지 216 다운로드 완료\n",
      "[005490] 페이지 217 다운로드 완료\n",
      "[005490] 페이지 218 다운로드 완료\n",
      "[005490] 페이지 219 다운로드 완료\n",
      "[005490] 페이지 220 다운로드 완료\n",
      "[005490] 페이지 221 다운로드 완료\n",
      "[005490] 페이지 222 다운로드 완료\n",
      "[005490] 페이지 223 다운로드 완료\n",
      "[005490] 페이지 224 다운로드 완료\n",
      "[005490] 페이지 225 다운로드 완료\n",
      "[005490] 페이지 226 다운로드 완료\n",
      "[005490] 페이지 227 다운로드 완료\n",
      "[005490] 페이지 228 다운로드 완료\n",
      "[005490] 페이지 229 다운로드 완료\n",
      "[005490] 페이지 230 다운로드 완료\n",
      "[005490] 페이지 231 다운로드 완료\n",
      "[005490] 페이지 232 다운로드 완료\n",
      "[005490] 페이지 233 다운로드 완료\n",
      "[005490] 페이지 234 다운로드 완료\n",
      "[005490] 페이지 235 다운로드 완료\n",
      "[005490] 페이지 236 다운로드 완료\n",
      "[005490] 페이지 237 다운로드 완료\n",
      "[005490] 페이지 238 다운로드 완료\n",
      "[005490] 페이지 239 다운로드 완료\n",
      "[005490] 페이지 240 다운로드 완료\n",
      "[005490] 페이지 241 다운로드 완료\n",
      "[005490] 페이지 242 다운로드 완료\n",
      "[005490] 페이지 243 다운로드 완료\n",
      "[005490] 페이지 244 다운로드 완료\n",
      "[005490] 페이지 245 다운로드 완료\n",
      "[005490] 페이지 246 다운로드 완료\n",
      "[005490] 페이지 247 다운로드 완료\n",
      "[005490] 페이지 248 다운로드 완료\n",
      "[005490] 페이지 249 다운로드 완료\n",
      "[005490] 페이지 250 다운로드 완료\n",
      "[005490] 페이지 251 다운로드 완료\n",
      "[005490] 페이지 252 다운로드 완료\n",
      "[005490] 페이지 253 다운로드 완료\n",
      "[005490] 페이지 254 다운로드 완료\n",
      "[005490] 페이지 255 다운로드 완료\n",
      "[005490] 페이지 256 다운로드 완료\n",
      "[005490] 페이지 257 다운로드 완료\n",
      "[005490] 페이지 258 다운로드 완료\n",
      "[005490] 페이지 259 다운로드 완료\n",
      "[005490] 페이지 260 다운로드 완료\n",
      "[005490] 페이지 261 다운로드 완료\n",
      "[005490] 페이지 262 다운로드 완료\n",
      "[005490] 페이지 263 다운로드 완료\n",
      "[005490] 페이지 264 다운로드 완료\n",
      "[005490] 페이지 265 다운로드 완료\n",
      "[005490] 페이지 266 다운로드 완료\n",
      "[005490] 페이지 267 다운로드 완료\n",
      "[005490] 페이지 268 다운로드 완료\n",
      "[005490] 페이지 269 다운로드 완료\n",
      "[005490] 페이지 270 다운로드 완료\n",
      "[005490] 페이지 271 다운로드 완료\n",
      "[005490] 페이지 272 다운로드 완료\n",
      "[005490] 페이지 273 다운로드 완료\n",
      "[005490] 페이지 274 다운로드 완료\n",
      "[005490] 페이지 275 다운로드 완료\n",
      "[005490] 페이지 276 다운로드 완료\n",
      "[005490] 페이지 277 다운로드 완료\n",
      "[005490] 페이지 278 다운로드 완료\n",
      "[005490] 페이지 279 다운로드 완료\n",
      "[005490] 페이지 280 다운로드 완료\n",
      "[005490] 페이지 281 다운로드 완료\n",
      "[005490] 페이지 282 다운로드 완료\n",
      "[005490] 페이지 283 다운로드 완료\n",
      "[005490] 페이지 284 다운로드 완료\n",
      "[005490] 페이지 285 다운로드 완료\n",
      "[005490] 페이지 286 다운로드 완료\n",
      "[005490] 페이지 287 다운로드 완료\n",
      "[005490] 페이지 288 다운로드 완료\n",
      "[005490] 페이지 289 다운로드 완료\n",
      "[005490] 페이지 290 다운로드 완료\n",
      "[005490] 페이지 291 다운로드 완료\n",
      "[005490] 페이지 292 다운로드 완료\n",
      "[005490] 페이지 293 다운로드 완료\n",
      "[005490] 페이지 294 다운로드 완료\n",
      "[005490] 페이지 295 다운로드 완료\n",
      "[005490] 페이지 296 다운로드 완료\n",
      "[005490] 페이지 297 다운로드 완료\n",
      "[005490] 페이지 298 다운로드 완료\n",
      "[005490] 페이지 299 다운로드 완료\n",
      "[005490] 페이지 300 다운로드 완료\n",
      "[005490] 더 이상 뉴스가 없습니다.\n",
      "[005490] 뉴스 데이터 저장 완료: ./종목별_뉴스\\005490_뉴스_20231209_20240308.csv\n",
      "\n",
      "247540 종목 뉴스 크롤링 시작...\n",
      "[247540] 페이지 1 다운로드 완료\n",
      "[247540] 페이지 2 다운로드 완료\n",
      "[247540] 페이지 3 다운로드 완료\n",
      "[247540] 페이지 4 다운로드 완료\n",
      "[247540] 페이지 5 다운로드 완료\n",
      "[247540] 페이지 6 다운로드 완료\n",
      "[247540] 페이지 7 다운로드 완료\n",
      "[247540] 페이지 8 다운로드 완료\n",
      "[247540] 페이지 9 다운로드 완료\n",
      "[247540] 더 이상 뉴스가 없습니다.\n",
      "[247540] 뉴스 데이터 저장 완료: ./종목별_뉴스\\247540_뉴스_20231209_20240308.csv\n",
      "\n",
      "모든 종목의 뉴스 데이터 크롤링 및 저장 완료!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
