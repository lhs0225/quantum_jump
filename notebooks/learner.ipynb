{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "34fe937a-965e-45c4-b3ef-484b3aafd9e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 01:17:46.372313: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-15 01:17:47.124713: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM, Conv1D,BatchNormalization, Dropout, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import SGD,Adam\n",
    "from tensorflow.keras import backend\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import abc\n",
    "import collections\n",
    "import threading\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "\n",
    "from mplfinance.original_flavor import candlestick_ohlc\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73b4c7a8-f8e3-42fe-bae3-81351aba3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "# 날짜, 시간 관련 문자열 형식\n",
    "FORMAT_DATE = \"%Y%m%d\"\n",
    "FORMAT_DATETIME = \"%Y%m%d%H%M%S\"\n",
    "CHART_DATA_COLUMNS = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "def get_time_str():\n",
    "    return datetime.datetime.fromtimestamp(\n",
    "        int(time.time())).strftime(FORMAT_DATETIME)\n",
    "\n",
    "def sigmoid(x):\n",
    "    x = max(min(x, 10), -10)\n",
    "    return 1. / (1. + np.exp(-x))\n",
    "\n",
    "def softmax(x):\n",
    "    e_x=np.exp(x -np.max(x))\n",
    "    return e_x/e_x.sum(axis=0)\n",
    "\n",
    "#settings\n",
    "# 로거 이름\n",
    "LOGGER_NAME = 'rltrader'\n",
    "# 경로 설정\n",
    "__file__='./'       \n",
    "BASE_DIR = os.environ.get('RLTRADER_BASE', os.path.abspath(os.path.join(__file__)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f31aa178-0ebf-4567-b034-21e59829f88f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CHART_DATA_COLUMNS = ['datetime', 'open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "class load_data:\n",
    "    #이미 전처리 된놈 불러올거임\n",
    "    def __init__(self):\n",
    "        self.chart_data=None \n",
    "        self.train_data=None\n",
    "        \n",
    "    def train_csv(self,code=None,start_day=None,end_day=None):\n",
    "        df=pd.read_csv(f'./data/{code}_train.csv')\n",
    "        \n",
    "        chart_data=df[CHART_DATA_COLUMNS]\n",
    "        train_data=df.drop(['datetime', 'open', 'high', 'low', 'close'],axis=1)\n",
    "        return chart_data,train_data\n",
    "        \n",
    "    def predict_csv(self,code=None,start_day=None,end_day=None):\n",
    "        df=pd.read_csv(f'./data/{code}_predict.csv')\n",
    "        \n",
    "        chart_data=df[CHART_DATA_COLUMNS]\n",
    "        train_data=df.drop(['datetime', 'open', 'high', 'low', 'close'],axis=1)\n",
    "        return chart_data,train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a8a2d5-8ef1-4052-af39-42282d15c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    PRICE_IDX = 4  # 차트데이터에서 종가의 위치\n",
    "\n",
    "    def __init__(self, chart_data=None):\n",
    "        self.chart_data = chart_data\n",
    "        self.observation = None  #에이전트에게 전달한 (분봉,일봉) 차트정보\n",
    "        self.idx = -1\n",
    "\n",
    "    def reset(self):  #관찰 초기화 처음으로 이동\n",
    "        self.observation = None\n",
    "        self.idx = -1\n",
    "\n",
    "    def observe(self):  #다음 관찰\n",
    "        if len(self.chart_data) > (self.idx + 1):\n",
    "            self.idx += 1\n",
    "            self.observation = self.chart_data.iloc[self.idx]\n",
    "            return self.observation\n",
    "        return None\n",
    "\n",
    "    def get_price(self):  #현재 종가 반환\n",
    "        if self.observation is not None:\n",
    "            return self.observation.iloc[self.PRICE_IDX]\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a410c2f-ce7a-438d-8c5a-e28763b1c34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    # 에이전트 상태가 구성하는 값 개수\n",
    "    # 주식 보유 비율, 손익률, 주당 매수 단가 대비 주가 등락률\n",
    "    STATE_DIM = 3\n",
    "\n",
    "    # 매매 수수료 및 세금\n",
    "    TRADING_CHARGE = 0.00015  # 거래 수수료 0.015% +\n",
    "    TRADING_TAX = 0.0018  # 거래세 0.18%\n",
    "    TRADING_HOGA = 0.0017  # 호가 0.17%\n",
    "\n",
    "    # 행동\n",
    "    ACTION_BUY = 0  # 매수\n",
    "    ACTION_SELL = 1  # 매도\n",
    "    ACTION_HOLD = 2  # 관망\n",
    "    # 인공 신경망에서 확률을 구할 행동들\n",
    "    ACTIONS = [ACTION_BUY, ACTION_SELL, ACTION_HOLD]\n",
    "    NUM_ACTIONS = len(ACTIONS) #3  # 인공 신경망에서 고려할 출력값의 개수\n",
    "\n",
    "    def __init__(self, environment, initial_balance, min_trading_price, max_trading_price):\n",
    "        # 현재 주식 가격을 가져오기 위해 환경 참조\n",
    "        self.environment = environment   #Environment(chart_data) 클래스\n",
    "        self.initial_balance = initial_balance  # 초기 자본금\n",
    "\n",
    "        self.min_trading_price = min_trading_price # 최소 단일 매매 금액\n",
    "        self.max_trading_price = max_trading_price # 최대 단일 매매 금액\n",
    "\n",
    "        # Agent 클래스의 속성\n",
    "        self.balance = initial_balance  # 현재 현금 잔고\n",
    "        self.num_stocks = 0  # 보유 주식 수\n",
    "        self.portfolio_value = 0 # 포트폴리오 가치: balance + num_stocks * {현재 주식 가격}\n",
    "\n",
    "        self.num_buy = 0  # 매수 횟수\n",
    "        self.num_sell = 0  # 매도 횟수\n",
    "        self.num_hold = 0  # 관망 횟수\n",
    "\n",
    "        # Agent 클래스의 상태  STATE_DIM=3\n",
    "        self.ratio_hold = 0  # 주식 보유 비율 (내자산에서 주식으로 들고있는 비율 주식수*종가 /pv)\n",
    "        self.profitloss = 0  # 손익률\n",
    "        self.avg_buy_price = 0  # 주당 매수 단가\n",
    "\n",
    "    def reset(self):  #다음 에피소드를 위해 초기화\n",
    "        self.balance = self.initial_balance\n",
    "        self.num_stocks = 0\n",
    "        self.portfolio_value = self.initial_balance\n",
    "        self.num_buy = 0\n",
    "        self.num_sell = 0\n",
    "        self.num_hold = 0\n",
    "        self.ratio_hold = 0\n",
    "        self.profitloss = 0\n",
    "        self.avg_buy_price = 0\n",
    "\n",
    "        #에이전트의 상태 반환  train_data에 추가됨\n",
    "    def get_states(self):\n",
    "        self.ratio_hold = self.num_stocks * self.environment.get_price() \\\n",
    "            / self.portfolio_value\n",
    "        return (\n",
    "            self.ratio_hold,\n",
    "            self.profitloss,\n",
    "            (self.environment.get_price() / self.avg_buy_price) - 1 \\\n",
    "                if self.avg_buy_price > 0 else 0\n",
    "        )\n",
    "\n",
    "    def decide_action(self, pred_value, pred_policy, epsilon):\n",
    "        confidence = 0.\n",
    "\n",
    "        pred = pred_policy\n",
    "        if pred is None:\n",
    "            pred = pred_value\n",
    "\n",
    "        if pred is None:\n",
    "            epsilon = 1     #정책, 가치 순으로 값이 없으면 탐험\n",
    "        else:\n",
    "            #매수,매도,관망의 예측값이 모두 같은 경우 탐험\n",
    "            maxpred = np.max(pred)\n",
    "            if (pred == maxpred).all():\n",
    "                epsilon = 1\n",
    "\n",
    "        # 탐험 결정\n",
    "        if np.random.rand() < epsilon:\n",
    "            exploration = True\n",
    "            action = np.random.randint(self.NUM_ACTIONS)  #탐험일경우 무작위 행동\n",
    "        else:\n",
    "            exploration = False\n",
    "            action = np.argmax(pred)\n",
    "\n",
    "        confidence = .5\n",
    "        if pred_policy is not None:\n",
    "            confidence = pred[action]\n",
    "        elif pred_value is not None:\n",
    "            confidence = sigmoid(pred[action])\n",
    "\n",
    "        return action, confidence, exploration\n",
    "\n",
    "    def validate_action(self, action):\n",
    "        if action == Agent.ACTION_BUY:\n",
    "            # 적어도 1주를 살 수 있는지 확인\n",
    "            if self.balance < self.environment.get_price()*(1+self.TRADING_HOGA) * (1 + self.TRADING_CHARGE):\n",
    "                return False\n",
    "        elif action == Agent.ACTION_SELL:\n",
    "            # 주식 잔고가 있는지 확인\n",
    "            if self.num_stocks <= 0:\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def decide_trading_unit(self, confidence):\n",
    "        if np.isnan(confidence):\n",
    "            return self.min_trading_price\n",
    "        added_trading_price = max(min(int(confidence * (self.max_trading_price - self.min_trading_price))\n",
    "                                      ,self.max_trading_price-self.min_trading_price), 0)\n",
    "        trading_price = self.min_trading_price + added_trading_price\n",
    "        return max(int(trading_price / self.environment.get_price()), 1)  #구매할 주식수 리턴\n",
    "\n",
    "    def act(self, action, confidence):\n",
    "        if not self.validate_action(action):\n",
    "            action = Agent.ACTION_HOLD\n",
    "\n",
    "        # 환경에서 현재 가격 얻기\n",
    "        curr_price = self.environment.get_price()\n",
    "\n",
    "        # 매수\n",
    "        if action == Agent.ACTION_BUY:\n",
    "            # 매수할 단위를 판단\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            balance = (\n",
    "                self.balance - curr_price *(1+self.TRADING_HOGA)*(1 + self.TRADING_CHARGE) * trading_unit\n",
    "            )\n",
    "            # 보유 현금이 모자랄 경우 보유 현금으로 가능한 만큼 최대한 매수\n",
    "            if balance < 0:\n",
    "                trading_unit = min(\n",
    "                    int(self.balance / (curr_price *(1+self.TRADING_HOGA) * (1 + self.TRADING_CHARGE))),\n",
    "                    int(self.max_trading_price / curr_price)\n",
    "                )\n",
    "            # 수수료를 적용하여 총 매수 금액 산정\n",
    "            invest_amount = curr_price *(1+self.TRADING_HOGA) * (1 + self.TRADING_CHARGE) * trading_unit\n",
    "            if invest_amount > 0:\n",
    "                self.avg_buy_price = \\\n",
    "                    (self.avg_buy_price * self.num_stocks + (curr_price *(1+self.TRADING_HOGA) * (1 + self.TRADING_CHARGE) )* trading_unit) \\\n",
    "                        / (self.num_stocks + trading_unit)  # 주당 매수 단가 갱신\n",
    "                self.balance -= invest_amount  # 보유 현금을 갱신\n",
    "                self.num_stocks += trading_unit  # 보유 주식 수를 갱신\n",
    "                self.num_buy += 1  # 매수 횟수 증가\n",
    "\n",
    "        # 매도\n",
    "        elif action == Agent.ACTION_SELL:\n",
    "            # 매도할 단위를 판단\n",
    "            trading_unit = self.decide_trading_unit(confidence)\n",
    "            # 보유 주식이 모자랄 경우 가능한 만큼 최대한 매도\n",
    "            trading_unit = min(trading_unit, self.num_stocks)  #trading_unit은 num_stock보다 작거나 같다.\n",
    "            # 매도\n",
    "            invest_amount = curr_price*(1-self.TRADING_HOGA) * (1 - (self.TRADING_TAX + self.TRADING_CHARGE)) * trading_unit\n",
    "            if invest_amount > 0:\n",
    "                # 주당 매수 단가 갱신\n",
    "                self.avg_buy_price = \\\n",
    "                    (self.avg_buy_price * self.num_stocks - (curr_price*(1-self.TRADING_HOGA) * (1 - (self.TRADING_TAX + self.TRADING_CHARGE))) * trading_unit) \\\n",
    "                        / (self.num_stocks - trading_unit) \\\n",
    "                            if self.num_stocks > trading_unit else 0\n",
    "                self.num_stocks -= trading_unit  # 보유 주식 수를 갱신\n",
    "                self.balance += invest_amount  # 보유 현금을 갱신\n",
    "                self.num_sell += 1  # 매도 횟수 증가\n",
    "\n",
    "        # 관망\n",
    "        elif action == Agent.ACTION_HOLD:\n",
    "            self.num_hold += 1  # 관망 횟수 증가\n",
    "\n",
    "        # 포트폴리오 가치 갱신\n",
    "        self.portfolio_value = self.balance + curr_price * self.num_stocks\n",
    "        self.profitloss = self.portfolio_value / self.initial_balance - 1\n",
    "        return self.profitloss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "90c87778-c854-455b-8355-d0cb983c5934",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualizer:\n",
    "    COLORS = ['r', 'b', 'g']  #매수, 매도, 관망\n",
    "\n",
    "    def __init__(self):\n",
    "        self.canvas = None\n",
    "        # 캔버스 같은 역할을 하는 Matplotlib의 Figure 클래스 객체\n",
    "        self.fig = None\n",
    "        # 차트를 그리기 위한 Matplotlib의 Axes 클래스 객체\n",
    "        self.axes = None\n",
    "        self.title = ''  # 그림 제목\n",
    "        self.x = []\n",
    "        self.xticks = []\n",
    "        self.xlabels = []\n",
    "\n",
    "    def prepare(self, chart_data, title):\n",
    "        self.title = title\n",
    "        with lock:\n",
    "            # 캔버스를 초기화하고 5개의 차트를 그릴 준비\n",
    "            self.fig, self.axes = plt.subplots(\n",
    "                nrows=5, ncols=1, facecolor='w', sharex=True, figsize=(50, 15))\n",
    "\n",
    "            for ax in self.axes:\n",
    "                # 보기 어려운 과학적 표기 비활성화\n",
    "                ax.get_xaxis().get_major_formatter() \\\n",
    "                    .set_scientific(False)\n",
    "                ax.get_yaxis().get_major_formatter() \\\n",
    "                    .set_scientific(False)\n",
    "                # y axis 위치 오른쪽으로 변경\n",
    "                ax.yaxis.tick_right()\n",
    "            # 차트 1. 일봉 차트\n",
    "            self.axes[0].set_ylabel('Env.')  # y 축 레이블 표시\n",
    "            x = np.arange(len(chart_data))\n",
    "            # open, high, low, close 순서로된 2차원 배열\n",
    "            ohlc = np.hstack((\n",
    "                x.reshape(-1, 1), np.array(chart_data)[:, 1:-1]))\n",
    "            # 양봉은 빨간색으로 음봉은 파란색으로 표시\n",
    "            candlestick_ohlc(self.axes[0], ohlc, colorup='r', colordown='b')\n",
    "            # 거래량 가시화\n",
    "            ax = self.axes[0].twinx()\n",
    "            volume = np.array(chart_data)[:, -1].tolist()\n",
    "            ax.bar(x, volume, color='b', alpha=0.3)\n",
    "            # x축 설정\n",
    "            self.x = np.arange(len(chart_data['datetime']))\n",
    "            self.xticks = chart_data.index[[0, -1]]\n",
    "            self.xlabels = chart_data.iloc[[0, -1]]['datetime']\n",
    "\n",
    "    def plot(self, epoch_str=None, num_epoches=None, epsilon=None,\n",
    "            action_list=None, actions=None, num_stocks=None,\n",
    "            outvals_value=[], outvals_policy=[], exps=None,\n",
    "            initial_balance=None, pvs=None):\n",
    "        with lock:\n",
    "            actions = np.array(actions)  # 에이전트의 행동 배열\n",
    "            # 가치 신경망의 출력 배열\n",
    "            outvals_value = np.array(outvals_value)\n",
    "            # 정책 신경망의 출력 배열\n",
    "            outvals_policy = np.array(outvals_policy)\n",
    "            # 초기 자본금 배열\n",
    "            pvs_base = np.zeros(len(actions)) + initial_balance\n",
    "\n",
    "            # 차트 2. 에이전트 상태 (행동, 보유 주식 수)\n",
    "            for action, color in zip(action_list, self.COLORS):\n",
    "                for i in self.x[actions == action]:\n",
    "                    # 배경 색으로 행동 표시\n",
    "                    self.axes[1].axvline(i, color=color, alpha=0.1)\n",
    "            self.axes[1].plot(self.x, num_stocks, '-k')  # 보유 주식 수 그리기\n",
    "\n",
    "            # 차트 3. 가치 신경망\n",
    "            if len(outvals_value) > 0:\n",
    "                max_actions = np.argmax(outvals_value, axis=1)\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    # 배경 그리기\n",
    "                    for idx in self.x:\n",
    "                        if max_actions[idx] == action:\n",
    "                            self.axes[2].axvline(idx, color=color, alpha=0.1)\n",
    "                    # 가치 신경망 출력 그리기\n",
    "                    self.axes[2].plot(self.x, outvals_value[:, action],\n",
    "                        color=color, linestyle='-')\n",
    "\n",
    "            # 차트 4. 정책 신경망\n",
    "            # 탐험을 노란색 배경으로 그리기\n",
    "            for exp_idx in exps:\n",
    "                self.axes[3].axvline(exp_idx, color='y')\n",
    "            # 행동을 배경으로 그리기\n",
    "            _outvals = outvals_policy if len(outvals_policy) > 0 else outvals_value\n",
    "            for idx, outval in zip(self.x, _outvals):\n",
    "                color = 'white'\n",
    "                if np.isnan(outval.max()):\n",
    "                    continue\n",
    "                if outval.argmax() == Agent.ACTION_BUY:\n",
    "                    color = self.COLORS[0]  # 매수 빨간색\n",
    "                elif outval.argmax() == Agent.ACTION_SELL:\n",
    "                    color = self.COLORS[1]  # 매도 파란색\n",
    "                elif outval.argmax() == Agent.ACTION_HOLD:\n",
    "                    color = self.COLORS[2]  # 관망 초록색\n",
    "                self.axes[3].axvline(idx, color=color, alpha=0.1)\n",
    "            # 정책 신경망의 출력 그리기\n",
    "            if len(outvals_policy) > 0:\n",
    "                for action, color in zip(action_list, self.COLORS):\n",
    "                    self.axes[3].plot(\n",
    "                        self.x, outvals_policy[:, action],\n",
    "                        color=color, linestyle='-')\n",
    "\n",
    "            # 차트 5. 포트폴리오 가치\n",
    "            self.axes[4].axhline(\n",
    "                initial_balance, linestyle='-', color='gray')  #시작금액 기준선\n",
    "            self.axes[4].fill_between(self.x, pvs, pvs_base,\n",
    "                where=pvs > pvs_base, facecolor='r', alpha=0.1) #수익\n",
    "            self.axes[4].fill_between(self.x, pvs, pvs_base,\n",
    "                where=pvs < pvs_base, facecolor='b', alpha=0.1) #손해\n",
    "            self.axes[4].plot(self.x, pvs, '-k')\n",
    "            self.axes[4].xaxis.set_ticks(self.xticks)\n",
    "            self.axes[4].xaxis.set_ticklabels(self.xlabels)\n",
    "\n",
    "            # 에포크 및 탐험 비율\n",
    "            self.fig.suptitle(f'{self.title}\\nEPOCH:{epoch_str}/{num_epoches} EPSILON:{epsilon:.2f}')\n",
    "            # 캔버스 레이아웃 조정\n",
    "            self.fig.tight_layout()\n",
    "            self.fig.subplots_adjust(top=0.85)\n",
    "\n",
    "    def clear(self, xlim):\n",
    "        with lock:\n",
    "            _axes = self.axes.tolist()\n",
    "            for ax in _axes[1:]:\n",
    "                ax.cla()  # 그린 차트 지우기\n",
    "                ax.relim()  # limit를 초기화\n",
    "                ax.autoscale()  # 스케일 재설정\n",
    "            # y축 레이블 재설정\n",
    "            self.axes[1].set_ylabel('Agent')\n",
    "            self.axes[2].set_ylabel('V')\n",
    "            self.axes[3].set_ylabel('P')\n",
    "            self.axes[4].set_ylabel('PV')\n",
    "            for ax in _axes:\n",
    "                ax.set_xlim(xlim)  # x축 limit 재설정\n",
    "                ax.get_xaxis().get_major_formatter() \\\n",
    "                    .set_scientific(False)  # x축의 과학적 표기 비활성화\n",
    "                ax.get_yaxis().get_major_formatter() \\\n",
    "                    .set_scientific(False)  # y축의 과학적 표기 비활성화\n",
    "                # x축 간격을 일정하게 설정\n",
    "                ax.ticklabel_format(useOffset=False)\n",
    "\n",
    "    def save(self, path):\n",
    "        with lock:\n",
    "            self.fig.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b934272e-4b06-49af-ae75-3df131509dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network:\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    def __init__(self, input_dim=0, output_dim=0, lr=0.001,\n",
    "                shared_network=None, activation='sigmoid', loss='mse'):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.shared_network = shared_network\n",
    "        self.activation = activation\n",
    "        self.loss = loss\n",
    "        self.model = None\n",
    "\n",
    "    def predict(self, sample):\n",
    "        with self.lock:\n",
    "            pred = self.model.predict_on_batch(sample).flatten()\n",
    "            return pred\n",
    "\n",
    "    def train_on_batch(self, x, y):\n",
    "        loss = 0.\n",
    "        with self.lock:\n",
    "            history = self.model.fit(x, y, epochs=10, verbose=False)\n",
    "            loss += np.sum(history.history['loss'])\n",
    "        return loss\n",
    "\n",
    "    def save_model(self, model_path):\n",
    "        if model_path is not None and self.model is not None:\n",
    "            self.model.save_weights(model_path, overwrite=True)\n",
    "\n",
    "    def load_model(self, model_path):\n",
    "        if model_path is not None:\n",
    "            self.model.load_weights(model_path)\n",
    "\n",
    "    @classmethod\n",
    "    def get_shared_network(cls, net='lstm', num_steps=1, input_dim=0, output_dim=0):\n",
    "        # output_dim은 pytorch에서 필요\n",
    "        if net == 'lstm':\n",
    "            return LSTMNetwork.get_network_head(Input((num_steps, input_dim)))\n",
    "\n",
    "class LSTMNetwork(Network):\n",
    "    def __init__(self, *args, num_steps=1, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_steps = num_steps\n",
    "        inp = None\n",
    "        output = None\n",
    "        if self.shared_network is None:\n",
    "            inp = Input((self.num_steps, self.input_dim))\n",
    "            output = self.get_network_head(inp).output\n",
    "        else:\n",
    "            inp = self.shared_network.input\n",
    "            output = self.shared_network.output\n",
    "        output = Dense(\n",
    "            self.output_dim, activation=self.activation,\n",
    "            kernel_initializer='random_normal')(output)\n",
    "        self.model = Model(inp, output)\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=self.lr), loss=self.loss)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_network_head(inp):\n",
    "        output = LSTM(256, dropout=0.1, return_sequences=True,\n",
    "                    kernel_initializer='random_normal')(inp)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = LSTM(128, dropout=0.1, return_sequences=True,\n",
    "                    kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = LSTM(64, dropout=0.1, return_sequences=True,\n",
    "                    kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        output = LSTM(32, dropout=0.1, kernel_initializer='random_normal')(output)\n",
    "        output = BatchNormalization()(output)\n",
    "        return Model(inp, output)\n",
    "\n",
    "    def train_on_batch(self, x, y):\n",
    "        x = np.array(x).reshape((-1, self.num_steps, self.input_dim))\n",
    "        return super().train_on_batch(x, y)\n",
    "\n",
    "    def predict(self, sample):\n",
    "        sample = np.array(sample).reshape((1, self.num_steps, self.input_dim))\n",
    "        return super().predict(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e7745a97-4d81-4d93-b56c-4eb8a9bc0080",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(LOGGER_NAME)\n",
    "\n",
    "class ReinforcementLearner:\n",
    "    __metaclass__ = abc.ABCMeta\n",
    "    lock = threading.Lock()\n",
    "\n",
    "    def __init__(self, rl_method='rl', stock_code=None,\n",
    "                chart_data=None, training_data=None,\n",
    "                min_trading_price=100000, max_trading_price=10000000,\n",
    "                net='lstm', num_steps=1, lr=0.0005,\n",
    "                discount_factor=0.9, num_epoches=1000,\n",
    "                balance=100000000, start_epsilon=1,\n",
    "                value_network=None, policy_network=None,\n",
    "                value_network_activation='linear', policy_network_activation='softmax',\n",
    "                output_path='', reuse_models=True, gen_output=True):\n",
    "        # 인자 확인\n",
    "        assert min_trading_price > 0\n",
    "        assert max_trading_price > 0\n",
    "        assert max_trading_price >= min_trading_price\n",
    "        assert num_steps > 0\n",
    "        assert lr > 0\n",
    "        # 강화학습 설정\n",
    "        self.rl_method = rl_method\n",
    "        self.discount_factor = discount_factor\n",
    "        self.num_epoches = num_epoches\n",
    "        self.start_epsilon = start_epsilon\n",
    "        # 환경 설정\n",
    "        self.stock_code = stock_code\n",
    "        self.chart_data = chart_data\n",
    "        self.environment = Environment(chart_data)\n",
    "        # 에이전트 설정\n",
    "        self.agent = Agent(self.environment, balance, min_trading_price, max_trading_price)\n",
    "        # 학습 데이터\n",
    "        self.training_data = training_data\n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        # 벡터 크기 = 학습 데이터 벡터 크기 + 에이전트 상태 크기\n",
    "        self.num_features = self.agent.STATE_DIM\n",
    "        if self.training_data is not None:\n",
    "            self.num_features += self.training_data.shape[1]\n",
    "        # 신경망 설정\n",
    "        self.net = net\n",
    "        self.num_steps = num_steps\n",
    "        self.lr = lr\n",
    "        self.value_network = value_network\n",
    "        self.policy_network = policy_network\n",
    "        self.reuse_models = reuse_models\n",
    "        self.value_network_activation = value_network_activation\n",
    "        self.policy_network_activation = policy_network_activation\n",
    "        # 가시화 모듈\n",
    "        self.visualizer = Visualizer()\n",
    "        # 메모리\n",
    "        self.memory_sample = []\n",
    "        self.memory_action = []\n",
    "        self.memory_reward = []\n",
    "        self.memory_value = []\n",
    "        self.memory_policy = []\n",
    "        self.memory_pv = []\n",
    "        self.memory_num_stocks = []\n",
    "        self.memory_exp_idx = []\n",
    "        # 에포크 관련 정보\n",
    "        self.loss = 0.\n",
    "        self.itr_cnt = 0\n",
    "        self.exploration_cnt = 0\n",
    "        self.batch_size = 0\n",
    "        # 로그 등 출력 경로\n",
    "        self.epoch_summary_dir=None\n",
    "        self.output_path = output_path\n",
    "        self.gen_output = gen_output\n",
    "\n",
    "    def init_value_network(self, shared_network=None, loss='mse'):\n",
    "        if self.net == 'lstm':\n",
    "            self.value_network = LSTMNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=self.value_network_activation, loss=loss)\n",
    "        if self.reuse_models and os.path.exists(self.value_network_path):\n",
    "            print(os.path.exists(self.value_network_path))\n",
    "            self.value_network.load_model(model_path=self.value_network_path)\n",
    "\n",
    "    def init_policy_network(self, shared_network=None, loss='categorical_crossentropy'):\n",
    "        if self.net == 'lstm':\n",
    "            self.policy_network = LSTMNetwork(\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS,\n",
    "                lr=self.lr, num_steps=self.num_steps,\n",
    "                shared_network=shared_network,\n",
    "                activation=self.policy_network_activation, loss=loss)\n",
    "        if self.reuse_models and os.path.exists(self.policy_network_path):\n",
    "            print(os.path.exists(self.policy_network_path))\n",
    "            self.policy_network.load_model(model_path=self.policy_network_path)\n",
    "\n",
    "    def reset(self):\n",
    "        self.sample = None\n",
    "        self.training_data_idx = -1\n",
    "        # 환경 초기화\n",
    "        self.environment.reset()\n",
    "        # 에이전트 초기화\n",
    "        self.agent.reset()\n",
    "        # 가시화 초기화\n",
    "        self.visualizer.clear([0, len(self.chart_data)])\n",
    "        # 메모리 초기화\n",
    "        self.memory_sample = []\n",
    "        self.memory_action = []\n",
    "        self.memory_reward = []\n",
    "        self.memory_value = []\n",
    "        self.memory_policy = []\n",
    "        self.memory_pv = []\n",
    "        self.memory_num_stocks = []\n",
    "        self.memory_exp_idx = []\n",
    "        # 에포크 관련 정보 초기화\n",
    "        self.loss = 0.\n",
    "        self.itr_cnt = 0\n",
    "        self.exploration_cnt = 0\n",
    "        self.batch_size = 0\n",
    "\n",
    "    def build_sample(self):\n",
    "        self.environment.observe()\n",
    "        if len(self.training_data) > self.training_data_idx + 1:\n",
    "            self.training_data_idx += 1\n",
    "            self.sample = self.training_data.iloc[self.training_data_idx].tolist()\n",
    "            self.sample.extend(self.agent.get_states())\n",
    "            return self.sample  #train_data 와 현재 agent의 상태\n",
    "        return None\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_batch(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self):\n",
    "        # 배치 학습 데이터 생성\n",
    "        x, y_value, y_policy = self.get_batch()\n",
    "        # 손실 초기화\n",
    "        self.loss = None\n",
    "        if len(x) > 0:\n",
    "            loss = 0\n",
    "            if y_value is not None:\n",
    "                # 가치 신경망 갱신\n",
    "                loss += self.value_network.train_on_batch(x, y_value)\n",
    "            if y_policy is not None:\n",
    "                # 정책 신경망 갱신\n",
    "                loss += self.policy_network.train_on_batch(x, y_policy)\n",
    "            self.loss = loss\n",
    "\n",
    "    def visualize(self, epoch_str, num_epoches, epsilon):\n",
    "        self.memory_action = [Agent.ACTION_HOLD] * (self.num_steps - 1) + self.memory_action\n",
    "        self.memory_num_stocks = [0] * (self.num_steps - 1) + self.memory_num_stocks\n",
    "        if self.value_network is not None:\n",
    "            self.memory_value = [np.array([np.nan] * len(Agent.ACTIONS))] \\\n",
    "                                * (self.num_steps - 1) + self.memory_value\n",
    "        if self.policy_network is not None:\n",
    "            self.memory_policy = [np.array([np.nan] * len(Agent.ACTIONS))] \\\n",
    "                                * (self.num_steps - 1) + self.memory_policy\n",
    "        self.memory_pv = [self.agent.initial_balance] * (self.num_steps - 1) + self.memory_pv\n",
    "        self.visualizer.plot(\n",
    "            epoch_str=epoch_str, num_epoches=num_epoches,\n",
    "            epsilon=epsilon, action_list=Agent.ACTIONS,\n",
    "            actions=self.memory_action,\n",
    "            num_stocks=self.memory_num_stocks,\n",
    "            outvals_value=self.memory_value,\n",
    "            outvals_policy=self.memory_policy,\n",
    "            exps=self.memory_exp_idx,\n",
    "            initial_balance=self.agent.initial_balance,\n",
    "            pvs=self.memory_pv,\n",
    "        )\n",
    "        self.visualizer.save(os.path.join(self.epoch_summary_dir, f'epoch_summary_{epoch_str}.png'))\n",
    "\n",
    "    def run(self, learning=True):\n",
    "        info = (\n",
    "            f'[{self.stock_code}] RL:{self.rl_method} NET:{self.net} '\n",
    "            f'LR:{self.lr} DF:{self.discount_factor} '\n",
    "        )\n",
    "\n",
    "        with self.lock:\n",
    "            logger.debug(info)\n",
    "        # 시작 시간\n",
    "        time_start = time.time()\n",
    "\n",
    "        # 가시화 준비\n",
    "        # 차트 데이터는 변하지 않으므로 미리 가시화\n",
    "        print(f'{self.stock_code}__ 차트데이터 만들기')\n",
    "        self.visualizer.prepare(self.environment.chart_data, info)\n",
    "\n",
    "        # 가시화 결과 저장할 폴더 준비\n",
    "        if self.gen_output:\n",
    "            self.epoch_summary_dir = os.path.join(self.output_path, f'{self.stock_code}_visualize')\n",
    "            if not os.path.isdir(self.epoch_summary_dir):\n",
    "                os.makedirs(self.epoch_summary_dir)\n",
    "            else:\n",
    "                for f in os.listdir(self.epoch_summary_dir):\n",
    "                    os.remove(os.path.join(self.epoch_summary_dir, f))\n",
    "            ##===========================================================================================\n",
    "            if self.value_network_path is not None:\n",
    "                self.epoch_value_network_dir = \\\n",
    "                    os.path.join(self.value_network_path, f'{self.stock_code}_value_network')\n",
    "                if not os.path.isdir(self.epoch_value_network_dir):\n",
    "                    os.makedirs(self.epoch_value_network_dir)\n",
    "                else:\n",
    "                    for f in os.listdir(self.epoch_value_network_dir):\n",
    "                        os.remove(os.path.join(self.epoch_value_network_dir, f))\n",
    "            # ##===========================================================================================\n",
    "            if self.policy_network_path is not None:\n",
    "                self.epoch_policy_network_dir = \\\n",
    "                    os.path.join(self.policy_network_path, f'{self.stock_code}_policy_network')\n",
    "                if not os.path.isdir(self.epoch_policy_network_dir):\n",
    "                    os.makedirs(self.epoch_policy_network_dir)\n",
    "                else:\n",
    "                    for f in os.listdir(self.epoch_policy_network_dir):\n",
    "                        os.remove(os.path.join(self.epoch_policy_network_dir, f))\n",
    "\n",
    "\n",
    "        # 학습에 대한 정보 초기화\n",
    "        max_portfolio_value = 0\n",
    "        epoch_win_cnt = 0\n",
    "\n",
    "        # 에포크 반복\n",
    "        for epoch in tqdm(range(self.num_epoches)):\n",
    "\n",
    "            time_start_epoch = time.time()\n",
    "\n",
    "            # step 샘플을 만들기 위한 큐\n",
    "            q_sample = collections.deque(maxlen=self.num_steps) #num_step 크기만큼만 데이터를 저장\n",
    "\n",
    "            # 환경, 에이전트, 신경망, 가시화, 메모리 초기화\n",
    "            self.reset()\n",
    "\n",
    "            # 학습을 진행할 수록 탐험 비율 감소\n",
    "            if learning:\n",
    "                epsilon = self.start_epsilon * (1 - (epoch / (self.num_epoches - 1)))\n",
    "            else:\n",
    "                epsilon = self.start_epsilon\n",
    "\n",
    "            print(f'stock_code : {self.stock_code}__episode : {epoch}__에이전트 거래 시작')\n",
    "            for i in tqdm(range(len(self.training_data)), leave=False):\n",
    "                # 샘플 생성\n",
    "                next_sample = self.build_sample()  #train_data 와 현재 agent의 상태\n",
    "                if next_sample is None:\n",
    "                    break\n",
    "\n",
    "                # num_steps만큼 샘플 저장\n",
    "                q_sample.append(next_sample)\n",
    "                if len(q_sample) < self.num_steps:\n",
    "                    continue\n",
    "\n",
    "                # 가치, 정책 신경망 예측\n",
    "                pred_value = None\n",
    "                pred_policy = None\n",
    "                if self.value_network is not None:\n",
    "                    pred_value = self.value_network.predict(list(q_sample))\n",
    "                if self.policy_network is not None:\n",
    "                    pred_policy = self.policy_network.predict(list(q_sample))\n",
    "                # 신경망 또는 탐험에 의한 행동 결정\n",
    "                action, confidence, exploration = \\\n",
    "                    self.agent.decide_action(pred_value, pred_policy, epsilon)\n",
    "\n",
    "                # 결정한 행동을 수행하고 보상 획득\n",
    "                reward = self.agent.act(action, confidence) #self.profitloss = self.portfolio_value / self.initial_balance - 1\n",
    "\n",
    "                # 행동 및 행동에 대한 결과를 기억\n",
    "                self.memory_sample.append(list(q_sample))\n",
    "                self.memory_action.append(action)   #[매수, 매도, 관망] 중 한개\n",
    "                self.memory_reward.append(reward)\n",
    "                if self.value_network is not None:\n",
    "                    self.memory_value.append(pred_value)\n",
    "                if self.policy_network is not None:\n",
    "                    self.memory_policy.append(pred_policy)\n",
    "                self.memory_pv.append(self.agent.portfolio_value)\n",
    "                self.memory_num_stocks.append(self.agent.num_stocks)\n",
    "                if exploration:\n",
    "                    self.memory_exp_idx.append(self.training_data_idx)\n",
    "\n",
    "                # 반복에 대한 정보 갱신\n",
    "                self.batch_size += 1\n",
    "                self.itr_cnt += 1\n",
    "                self.exploration_cnt += 1 if exploration else 0\n",
    "\n",
    "            # 에포크 종료 후 학습\n",
    "            if learning:\n",
    "                self.fit()\n",
    "\n",
    "            # 에포크 관련 정보 로그 기록\n",
    "            num_epoches_digit = len(str(self.num_epoches))\n",
    "            epoch_str = str(epoch).rjust(num_epoches_digit, '0')\n",
    "            time_end_epoch = time.time()\n",
    "            elapsed_time_epoch = time_end_epoch - time_start_epoch\n",
    "            logger.debug(f'[{self.stock_code}][Epoch {epoch_str}/{self.num_epoches}] '\n",
    "                f'Epsilon:{epsilon:.4f} #Expl.:{self.exploration_cnt}/{self.itr_cnt} '\n",
    "                f'#Buy:{self.agent.num_buy} #Sell:{self.agent.num_sell} #Hold:{self.agent.num_hold} '\n",
    "                f'#Stocks:{self.agent.num_stocks} PV:{self.agent.portfolio_value:,.0f} '\n",
    "                f'Loss:{self.loss:.6f} ET:{elapsed_time_epoch:.4f}')\n",
    "\n",
    "            # 에포크 관련 정보 가시화\n",
    "            if self.gen_output:\n",
    "                    self.visualize(epoch_str, self.num_epoches, epsilon)\n",
    "\n",
    "            # 학습 관련 정보 갱신\n",
    "            max_portfolio_value = max(\n",
    "                max_portfolio_value, self.agent.portfolio_value)\n",
    "            if self.agent.portfolio_value > self.agent.initial_balance:\n",
    "                epoch_win_cnt += 1\n",
    "\n",
    "            # 에피소드별로 생성된 모델 저장\n",
    "            if self.value_network is not None and self.value_network_path is not None:\n",
    "                self.value_network.save_model(os.path.join(self.epoch_value_network_dir ,f'{self.stock_code}_value_network_{epoch}.h5'))\n",
    "            if self.policy_network is not None and self.policy_network_path is not None:\n",
    "                self.policy_network.save_model(os.path.join(self.epoch_policy_network_dir ,f'{self.stock_code}_policy_network_{epoch}.h5'))\n",
    "        #=======================================================================================\n",
    "\n",
    "        # 종료 시간\n",
    "        time_end = time.time()\n",
    "        elapsed_time = time_end - time_start\n",
    "\n",
    "        # 학습 관련 정보 로그 기록\n",
    "        with self.lock:\n",
    "            logger.debug(f'[{self.stock_code}] Elapsed Time:{elapsed_time:.4f} '\n",
    "                f'Max PV:{max_portfolio_value:,.0f} #Win:{epoch_win_cnt}')\n",
    "\n",
    "    def save_models(self):\n",
    "        if self.value_network is not None and self.value_network_path is not None:\n",
    "            self.value_network.save_model(self.value_network_path)\n",
    "        if self.policy_network is not None and self.policy_network_path is not None:\n",
    "            self.policy_network.save_model(self.policy_network_path)\n",
    "\n",
    "    def predict(self):\n",
    "        # 에이전트 초기화\n",
    "        self.agent.reset()\n",
    "        info = (\n",
    "            f'[{self.stock_code}] RL:{self.rl_method} NET:{self.net} '\n",
    "            f'LR:{self.lr} DF:{self.discount_factor} '\n",
    "        )\n",
    "        self.epoch_summary_dir = self.output_path\n",
    "        self.visualizer.prepare(self.environment.chart_data, info)\n",
    "        # step 샘플을 만들기 위한 큐\n",
    "        q_sample = collections.deque(maxlen=self.num_steps)\n",
    "        result = []\n",
    "        while True:\n",
    "            # 샘플 생성\n",
    "            next_sample = self.build_sample()\n",
    "            if next_sample is None:\n",
    "                break\n",
    "\n",
    "            # num_steps만큼 샘플 저장\n",
    "            q_sample.append(next_sample)\n",
    "            if len(q_sample) < self.num_steps:\n",
    "                continue\n",
    "\n",
    "            # 가치, 정책 신경망 예측\n",
    "            pred_value = None\n",
    "            pred_policy = None\n",
    "            if self.value_network is not None:\n",
    "                pred_value = self.value_network.predict(list(q_sample))\n",
    "            if self.policy_network is not None:\n",
    "                pred_policy = self.policy_network.predict(list(q_sample))\n",
    "\n",
    "            # 신경망 또는 탐험에 의한 행동 결정\n",
    "            action, confidence, exploration = self.agent.decide_action(pred_value, pred_policy, self.start_epsilon)\n",
    "\n",
    "            # 결정한 행동을 수행하고 보상 획득\n",
    "            reward = self.agent.act(action, confidence) #self.profitloss = self.portfolio_value / self.initial_balance - 1\n",
    "\n",
    "            # result.append([action,confidence,reward])\n",
    "            # 행동 및 행동에 대한 결과를 기억\n",
    "            self.memory_sample.append(list(q_sample))\n",
    "            self.memory_action.append(action)   #[매수, 매도, 관망] 중 한개\n",
    "            self.memory_reward.append(reward)\n",
    "            if self.value_network is not None:\n",
    "                self.memory_value.append(pred_value)\n",
    "            if self.policy_network is not None:\n",
    "                self.memory_policy.append(pred_policy)\n",
    "            self.memory_pv.append(self.agent.portfolio_value)\n",
    "            self.memory_num_stocks.append(self.agent.num_stocks)\n",
    "            if exploration:\n",
    "                self.memory_exp_idx.append(self.training_data_idx)\n",
    "\n",
    "        if self.gen_output:\n",
    "          self.visualize('predict', self.num_epoches, self.start_epsilon)\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8c8c344-0b9c-4a77-9c6f-f1847bec3a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, value_network_path=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.value_network_path = value_network_path\n",
    "        self.init_value_network()\n",
    "\n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),  #최신 경험부터 역순으로 접근하여 최근의 경험이 중요하게 작동하도록 함\n",
    "            reversed(self.memory_action),  #매수, 매도, 관망\n",
    "            reversed(self.memory_value),   #train data를 신경망으로 예측한 값\n",
    "            reversed(self.memory_reward),  #portfolio_value / self.initial_balance - 1\n",
    "        )\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        for i, (sample, action, value, reward) in enumerate(memory):\n",
    "            x[i] = sample\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            y_value[i] = value\n",
    "            y_value[i, action] = r + self.discount_factor * value_max_next  #바로전에 가장 좋았던 가치\n",
    "            value_max_next = value.max()\n",
    "        return x, y_value, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6774209-758b-4678-964f-c08c06cec1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCriticLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, shared_network=None,\n",
    "        value_network_path=None, policy_network_path=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        if shared_network is None:\n",
    "            self.shared_network = Network.get_shared_network(\n",
    "                net=self.net, num_steps=self.num_steps,\n",
    "                input_dim=self.num_features,\n",
    "                output_dim=self.agent.NUM_ACTIONS)\n",
    "        else:\n",
    "            self.shared_network = shared_network\n",
    "        self.value_network_path = value_network_path\n",
    "        self.policy_network_path = policy_network_path\n",
    "        if self.value_network is None:\n",
    "            self.init_value_network(shared_network=self.shared_network)\n",
    "        if self.policy_network is None:\n",
    "            self.init_policy_network(shared_network=self.shared_network)\n",
    "\n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        for i, (sample, action, value, policy, reward) in enumerate(memory):\n",
    "            x[i] = sample\n",
    "            r = self.memory_reward[-1] - reward\n",
    "            y_value[i, :] = value\n",
    "            y_value[i, action] = r + self.discount_factor * value_max_next\n",
    "            y_policy[i, :] = policy\n",
    "            y_policy[i, action] = softmax(r)\n",
    "            value_max_next = value.max()\n",
    "        return x, y_value, y_policy\n",
    "\n",
    "\n",
    "class A2CLearner(ActorCriticLearner):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def get_batch(self):\n",
    "        memory = zip(\n",
    "            reversed(self.memory_sample),\n",
    "            reversed(self.memory_action),\n",
    "            reversed(self.memory_value),\n",
    "            reversed(self.memory_policy),\n",
    "            reversed(self.memory_reward),\n",
    "        )\n",
    "        x = np.zeros((len(self.memory_sample), self.num_steps, self.num_features))\n",
    "        y_value = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        y_policy = np.zeros((len(self.memory_sample), self.agent.NUM_ACTIONS))\n",
    "        value_max_next = 0\n",
    "        reward_next = self.memory_reward[-1]\n",
    "        for i, (sample, action, value, policy, reward) in enumerate(memory):\n",
    "            x[i] = sample\n",
    "            r = reward_next + self.memory_reward[-1] - reward * 2\n",
    "            reward_next = reward\n",
    "            y_value[i, :] = value\n",
    "            y_value[i, action] = np.tanh(r + self.discount_factor * value_max_next)\n",
    "            advantage = y_value[i, action] - y_value[i].mean()\n",
    "            y_policy[i, :] = policy\n",
    "            y_policy[i, action] = softmax(advantage)\n",
    "            value_max_next = value.max()\n",
    "        return x, y_value, y_policy\n",
    "\n",
    "\n",
    "class A3CLearner(ReinforcementLearner):\n",
    "    def __init__(self, *args, list_stock_code=None,\n",
    "        list_chart_data=None, list_training_data=None,\n",
    "        list_min_trading_price=None, list_max_trading_price=None,\n",
    "        value_network_path=None, policy_network_path=None,\n",
    "        **kwargs):\n",
    "        assert len(list_training_data) > 0\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.num_features += list_training_data[0].shape[1]\n",
    "\n",
    "        # 공유 신경망 생성\n",
    "        self.shared_network = Network.get_shared_network(\n",
    "            net=self.net, num_steps=self.num_steps,\n",
    "            input_dim=self.num_features,\n",
    "            output_dim=self.agent.NUM_ACTIONS)\n",
    "        self.value_network_path = value_network_path\n",
    "        self.policy_network_path = policy_network_path\n",
    "        if self.value_network is None:\n",
    "            self.init_value_network(shared_network=self.shared_network)\n",
    "        if self.policy_network is None:\n",
    "            self.init_policy_network(shared_network=self.shared_network)\n",
    "\n",
    "        # A2CLearner 생성\n",
    "        self.learners = []\n",
    "        for (stock_code, chart_data, training_data,\n",
    "            min_trading_price, max_trading_price) in zip(\n",
    "                list_stock_code, list_chart_data, list_training_data,\n",
    "                list_min_trading_price, list_max_trading_price\n",
    "            ):\n",
    "            learner = A2CLearner(*args,\n",
    "                stock_code=stock_code, chart_data=chart_data,\n",
    "                training_data=training_data,\n",
    "                min_trading_price=min_trading_price,\n",
    "                max_trading_price=max_trading_price,\n",
    "                shared_network=self.shared_network,\n",
    "                value_network=self.value_network,\n",
    "                policy_network=self.policy_network, **kwargs)\n",
    "            self.learners.append(learner)\n",
    "\n",
    "    def run(self, learning=True):\n",
    "        threads = []\n",
    "        for learner in self.learners:\n",
    "            threads.append(threading.Thread(\n",
    "                target=learner.run, daemon=True, kwargs={'learning': learning}\n",
    "            ))\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        for thread in threads:\n",
    "            thread.join()\n",
    "\n",
    "    def predict(self):\n",
    "        threads = []\n",
    "        for learner in self.learners:\n",
    "            threads.append(threading.Thread(\n",
    "                target=learner.predict(), daemon=True\n",
    "            ))\n",
    "        for thread in threads:\n",
    "            thread.start()\n",
    "        for thread in threads:\n",
    "            thread.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b6b38335-6d6d-439a-9270-7b9816b368e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습기 파라미터 설정\n",
    "rl='a2c'\n",
    "net='lstm'\n",
    "code='247540'\n",
    "\n",
    "output_name = f'{code}_{get_time_str()}_{rl}_{net}'\n",
    "learning = 'train' in ['train', 'update']\n",
    "value_network_name = f'{code}_{get_time_str()}_{rl}_{net}_value'\n",
    "policy_network_name = f'{code}_{get_time_str()}_{rl}_{net}_policy'\n",
    "discount_factor=0.9\n",
    "start_epsilon = 0.5\n",
    "num_epoches = 2\n",
    "num_steps =  20\n",
    "balance=500000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6eb7553e-55bc-45cc-bfbf-a16fb068c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 경로 생성\n",
    "output_path = os.path.join(BASE_DIR, 'output', output_name)\n",
    "if not os.path.isdir(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# 모델 경로 준비\n",
    "value_network_path = os.path.join(BASE_DIR, 'models', value_network_name)\n",
    "policy_network_path = os.path.join(BASE_DIR, 'models', policy_network_name)\n",
    "\n",
    "# 로그 기록 설정\n",
    "log_path = os.path.join(output_path, f'{output_name}.log')\n",
    "if os.path.exists(log_path):\n",
    "    os.remove(log_path)\n",
    "logging.basicConfig(format='%(message)s')\n",
    "logger = logging.getLogger(LOGGER_NAME)\n",
    "logger.setLevel(logging.DEBUG)\n",
    "logger.propagate = False\n",
    "stream_handler = logging.StreamHandler(sys.stdout)\n",
    "stream_handler.setLevel(logging.INFO)\n",
    "file_handler = logging.FileHandler(filename=log_path, encoding='utf-8')\n",
    "file_handler.setLevel(logging.DEBUG)\n",
    "logger.addHandler(stream_handler)\n",
    "logger.addHandler(file_handler)\n",
    "\n",
    "        \n",
    "common_params = {}\n",
    "chart_data, training_data =load_data().train_csv(code)\n",
    "assert len(chart_data) >= num_steps\n",
    "\n",
    "# 최소/최대 단일 매매 금액 설정\n",
    "min_trading_price = 5000000\n",
    "max_trading_price = 50000000\n",
    "\n",
    "# 공통 파라미터 설정\n",
    "common_params={ 'rl_method':rl,\n",
    "               'stock_code':code, \n",
    "                'chart_data':chart_data,\n",
    "                'training_data':training_data,\n",
    "                'min_trading_price':min_trading_price, 'max_trading_price':max_trading_price, \n",
    "                'net':net,'num_steps':num_steps, 'lr':0.001, \n",
    "                'discount_factor':discount_factor,\n",
    "                'num_epoches':num_epoches,\n",
    "                'balance':balance,\n",
    "                'start_epsilon':start_epsilon,\n",
    "                'output_path':output_path,\n",
    "                'reuse_models':False\n",
    "              }\n",
    "\n",
    "# 강화학습 시작\n",
    "learner_A2C = None\n",
    "\n",
    "learner_A2C = A2CLearner(**{**common_params, \n",
    "                    'value_network_path': value_network_path, \n",
    "                    'policy_network_path': policy_network_path})\n",
    "\n",
    "assert learner_A2C is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c206d84-f52d-4a3e-9076-0c9ec02a1b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247540__ 차트데이터 만들기\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51444a07050d4661977797eb6b4ae74c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_code : 247540__episode : 0__에이전트 거래 시작\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-15 01:18:53.938042: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8907\n",
      "2024-03-15 01:19:01.143691: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x7f5d01f5c4f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-03-15 01:19:01.143755: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): NVIDIA GeForce GTX 1650 Ti, Compute Capability 7.5\n",
      "2024-03-15 01:19:01.199882: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:255] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-03-15 01:19:01.481035: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-03-15 01:19:01.610307: I ./tensorflow/compiler/jit/device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stock_code : 247540__episode : 1__에이전트 거래 시작\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lock = threading.Lock()\n",
    "learner_A2C.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69a93b18-4522-4d33-ba26-98853b0dbb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습된 모델 경로\n",
    "value_network_path = os.path.join(BASE_DIR, 'models', value_network_name\n",
    "                                  ,f'{code}_value_network',f'{code}_value_network_000.h5')\n",
    "policy_network_path = os.path.join(BASE_DIR, 'models', policy_network_name\n",
    "                                  ,f'{code}_policy_network',f'{code}_policy_network_000.h5')\n",
    "\n",
    "#학습된 모델로 예측\n",
    "chart_data, training_data =load_data().predict_csv(code)\n",
    "chart_data=chart_data[:20]\n",
    "training_data=training_data[:20]\n",
    "assert len(chart_data) >= num_steps\n",
    "\n",
    "# 최소/최대 단일 매매 금액 설정\n",
    "min_trading_price = 5000000\n",
    "max_trading_price = 50000000\n",
    "\n",
    "# 공통 파라미터 설정\n",
    "common_params={ 'rl_method':rl,\n",
    "               'stock_code':code, \n",
    "                'chart_data':chart_data,\n",
    "                'training_data':training_data,\n",
    "                'min_trading_price':min_trading_price, 'max_trading_price':max_trading_price, \n",
    "                'net':net,'num_steps':num_steps, 'lr':0.001, \n",
    "                'discount_factor':0.9,\n",
    "                'num_epoches':num_epoches,\n",
    "                'balance':50000000,\n",
    "                'start_epsilon':start_epsilon,\n",
    "                'output_path':output_path,\n",
    "                'reuse_models':True\n",
    "              }\n",
    "\n",
    "# 강화학습 시작\n",
    "learner_A2C = None\n",
    "\n",
    "learner_A2C = A2CLearner(**{**common_params, \n",
    "                    'value_network_path': value_network_path, \n",
    "                    'policy_network_path': policy_network_path})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c20e706d-62de-4c6c-837a-e474d5660b71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lock = threading.Lock()\n",
    "learner_A2C.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f75d0b8-83c4-4104-98b2-e91c5a8da954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
